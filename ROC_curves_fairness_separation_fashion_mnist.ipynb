{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8egQS_OXYOd"
   },
   "source": [
    "# ROC curves post processing method to improve fairness (particularly separation) in LS-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpwAMXEqXtP8"
   },
   "source": [
    "**Objectives**\n",
    "\n",
    "We want to emphasize unfairness in some AI/ML algorithm and then try to improve fairness of the machine by using post-correction process. Particularly with ROC curves. \n",
    "\n",
    "Therefore, we generate 2 Classes $C_1$ et $C_2$. Each class has a 2 groups : one majority and one minority. Thus, the protected attribute is being in a minority.\n",
    "\n",
    "The Goal is to show some misclassification of minority observations, and the correct it with ROC curves : the best operating point (the optimal pair of thresholds is hypothetically at the intersection of the 2 ROC curves. The next objective is to compute the threshold with the list of points we obtain by displaying both curves.\n",
    "\n",
    "For this notebook, we work on **real data (fashion-MNIST)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8pGlyBfYQBc"
   },
   "source": [
    "## ROC CURVES\n",
    "\n",
    "* abscissas : $P(\\hat{Y} = 1 \\mid Y = 0, A = a)$\n",
    "\n",
    "* ordinates : $P(\\hat{Y} = 1 \\mid Y = 1, A =a)$\n",
    "\n",
    "* Here, $A \\in ZOO$. $A=0$ means \"in majority\" and $A=1$ means \"in minority\"\n",
    "\n",
    "A point is plotted for a certain threshold $\\ \\xi$. The curve is the union of all points generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KL4HIeWMazCu"
   },
   "outputs": [],
   "source": [
    "# import the modules we need\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
    "from math import *\n",
    "pi = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsScGqcFXm6M"
   },
   "source": [
    "Fashion-MNIST\n",
    "\n",
    "**Labels**\n",
    "\n",
    "0 : T-Shirt / Top\n",
    "\n",
    "1 : Trouser\n",
    "\n",
    "2 : Pullover\n",
    "\n",
    "3 : Dress\n",
    "\n",
    "4 : Coat\n",
    "\n",
    "5 : Sandal\n",
    "\n",
    "6 : Shirt\n",
    "\n",
    "7 : Sneaker\n",
    "\n",
    "8 : Bag\n",
    "\n",
    "9 : Ankle Boot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Possible Classifications**\n",
    "\n",
    "N째1\n",
    "\n",
    "$C_1$ = { 0,  2} ; $C_2$ = { 1, 3}\n",
    "\n",
    "N째2\n",
    "\n",
    "$C_1$ = {0,6} ; $C_2$ = {7, 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuoT67jSazC2"
   },
   "source": [
    "## ROC curves methods to improve Fairness in ML : Focus on Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CgIhaFkdazC3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xi theorique =  0.125\n",
      "xi minorite =  0.15033557046979867\n",
      "xi majorite =  0.1906040268456376\n"
     ]
    }
   ],
   "source": [
    "#Probabilities belonging to each class. The 2 first ones are for C1. The 2 lasts are for C2. The sum must be equal to 1.\n",
    "cs = [6/16, 1/16, 7/16, 2/16] \n",
    "k = len(cs)  # nb of classes\n",
    "n = 4096 # nb of training samples\n",
    "n_test = 4096 # nb of testing samples\n",
    "\n",
    "gamma = 1  # regularization\n",
    "f = lambda t : np.exp(-t/2) # RBF kernel\n",
    "\n",
    "#xi_loop = list(np.linspace(0., 0.1, 200)) + list(np.linspace(0.1, 1, 25))\n",
    "xi_loop = list(np.linspace(-1, 0, 50)) + list(np.linspace(0, 0.2, 150)) + list(np.linspace(0.2, 1, 50))\n",
    "store_error = np.zeros( (len(xi_loop),4) , dtype = float )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "data_choice = 'MNIST'  # MNIST or Fashion-MNIST\n",
    "\n",
    "if data_choice == 'MNIST':\n",
    "    selected_labels=[7,9]\n",
    "    (init_data, init_labels), _ = mnist.load_data()\n",
    "\n",
    "if data_choice == 'fashion':\n",
    "    selected_labels=[8,9]\n",
    "    (init_data, init_labels), _ = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# get the data\n",
    "(init_data, init_labels), _ = fashion_mnist.load_data()\n",
    "\n",
    "# put data in good shape & get data details\n",
    "idx_init_labels = np.argsort(np.array(init_labels))    \n",
    "labels = init_labels[idx_init_labels]\n",
    "init_data = init_data[idx_init_labels,:,:]\n",
    "data = np.transpose(init_data.reshape(np.shape(init_data)[0],np.shape(init_data)[1]*np.shape(init_data)[2]))\n",
    "init_n,p=np.shape(data)[1],np.shape(data)[0]\n",
    "\n",
    "# N1 = [0, 2, 1, 3] or n2 = [0, 6, 7, 5]\n",
    "first_choice = [0, 2, 1, 3]\n",
    "second_choice = [0, 6, 7, 5]\n",
    "selected_labels = first_choice\n",
    "\n",
    "# put data between 0 and 1\n",
    "data = data/data.max()\n",
    "mean_data=np.mean(data,axis=1).reshape(len(data),1)\n",
    "\n",
    "# normalize data (CLT)\n",
    "norm2_data=0\n",
    "for i in range(init_n):\n",
    "    norm2_data+=1/init_n*np.linalg.norm(data[:,i]-mean_data)**2\n",
    "\n",
    "data=(data-mean_data)/np.sqrt(norm2_data)*np.sqrt(p)\n",
    "\n",
    "#select data we want considering their labels\n",
    "selected_data = []\n",
    "\n",
    "for i in range(len(selected_labels)):\n",
    "    selected_data.append(data[:,[ x for x in range(init_n) if labels[x]==selected_labels[i] ] ])\n",
    "    if i==0:    \n",
    "        cascade_selected_data = selected_data[-1]\n",
    "    else:\n",
    "        np.concatenate([cascade_selected_data,selected_data[-1]],axis=1)\n",
    "\n",
    "# recentering of the k classes\n",
    "mean_selected_data  = np.mean(cascade_selected_data,axis=1).reshape(len(cascade_selected_data),1)\n",
    "norm2_selected_data = np.mean(np.sum(np.power(np.abs(cascade_selected_data-mean_selected_data),2),0))\n",
    "\n",
    "for j in range(len(selected_labels)):\n",
    "    selected_data[j] = (selected_data[j] - mean_selected_data) / np.sqrt(norm2_selected_data) * np.sqrt(p)\n",
    "\n",
    "np.random.seed(928)\n",
    "for iter,xi in enumerate(xi_loop):\n",
    "\n",
    "    nb_average_loop = 30\n",
    "    store_output = np.zeros((nb_average_loop, 4))\n",
    "    for  average_index in range(nb_average_loop):\n",
    "        X=np.zeros( (p,n) )\n",
    "        X_test=np.zeros( (p,n_test) )\n",
    "        for i in range(k):\n",
    "            data = selected_data[i][ :, np.random.permutation(np.shape(selected_data[i])[1])]\n",
    "            #print(\"data = \", data)\n",
    "            X[ : , int(np.sum(cs[ : i])*n) : int(np.sum(cs[: i + 1]) * n)] = data[ : , : max(int(n*cs[i]), ceil(n * cs[i]))  ]\n",
    "            X_test[:,int(np.sum(cs[:i])*n_test):int(np.sum(cs[:i+1])*n_test)]=data[:,n:n+ max(int(n_test*cs[i]), ceil(n_test * cs[i]))  ] \n",
    "            #X_test[ : , int(np.sum(cs[ : i]) * n_test) : int(np.sum(cs[ : i + 1]) * n_test)] = data[ : , n + int(n_test * sum(cs[:i])) : n + int(n_test * sum(cs[: i + 1]))  ]\n",
    "\n",
    "        # kernel matrix\n",
    "        XX = X.T@X / p\n",
    "        K = f(-2 * XX + np.diag(XX).reshape(1, n)+np.diag(XX).reshape(n, 1))\n",
    "        #print(\"X = \", X)\n",
    "        #print(\"Xtest = \", X_test)\n",
    "\n",
    "        # target Y\n",
    "        y = np.concatenate([-np.ones(int((cs[0] + cs[1]) * n)),np.ones(int((cs[2] + cs[3]) * n))])\n",
    "\n",
    "        # target test Y_test\n",
    "        y_test = np.concatenate([-np.ones(int((cs[0] + cs[1]) * n_test)),np.ones(int((cs[2] + cs[3]) * n_test))])\n",
    "\n",
    "        # Q et Q^{-1}\n",
    "        inv_Q = K + n / gamma * np.eye(n)\n",
    "        Q_y = np.linalg.solve(inv_Q, y)\n",
    "        Q_1 = np.linalg.solve(inv_Q, np.ones(n))\n",
    "        \n",
    "        # alpha & b\n",
    "        b = np.sum(Q_y) / np.sum(Q_1)\n",
    "        alpha = Q_y - Q_1 * b\n",
    "\n",
    "        # classifcation/soft scores\n",
    "        g = lambda Y : alpha@f(np.diag(XX).reshape( (n, 1) ) + np.diag(Y.T@Y / p).reshape( (1,np.size(Y, 1)) ) - 2 * (X.T@Y / p)) + b\n",
    "        #g = lambda Y : alpha@f(np.diag(XX).reshape( (n,1) )+np.diag(Y.T@Y/p).reshape( (1,np.size(Y,1)) )-2*(X.T@Y/p))+b\n",
    "        g_test = g(X_test)\n",
    "        #for i in range(len(g_test)):\n",
    "            #if g_test[i] < 0:\n",
    "                #g_test[i] = -g_test[i]\n",
    "        #print(g_test)\n",
    "\n",
    "        # compute of False Alarm Rate (FAR) & Correct Detection Rate (CDR) for each class considering the sensitive/protected attribute\n",
    "        # \"Y / 킷  = 0\" <=> \"in C1\"\n",
    "        # \"Y / 킷 = 1\" <=> \"in C2\"\n",
    "        # m means minority\n",
    "        # M means majority\n",
    "        FAR_m = float(np.sum(g_test[  int(cs[0] * n_test) :  int(sum(cs[:2]) * n_test)] >= xi)) / float(ceil(cs[1] * n_test))\n",
    "        CDR_m = float( np.sum( g_test[ int((sum(cs[: - 1]) * n_test)) : ] >= xi)) / float(ceil(cs[3] * n_test))\n",
    "        \n",
    "        FAR_M = float( np.sum(  g_test[  : int(cs[0] * n_test)  ] >= xi)) /  float(int(cs[0] * n_test))\n",
    "        CDR_M = float( np.sum( g_test[ int(sum(cs[:2]) * n_test) : int(sum(cs[:3]) * n_test)] >= xi)) / float(int(cs[2] * n_test))\n",
    "\n",
    "        store_output[average_index, 0] = FAR_m\n",
    "        store_output[average_index, 1] = CDR_m\n",
    "        store_output[average_index, 2] = FAR_M\n",
    "        store_output[average_index, 3] = CDR_M\n",
    "\n",
    "\n",
    "    #print(store_output)\n",
    "    store_error[iter, :] = [ np.mean(store_output[ : , 0]), np.mean( store_output[ : , 1]), np.mean(store_output[:, 2]), np.mean(store_output[:, 3]) ]\n",
    "\n",
    "    \n",
    "def find_xi(labs, lord):\n",
    "  imin = 0\n",
    "  d2min = labs[0]**2 + (lord[0] - 1)**2\n",
    "  for i in range(1, len(labs)):\n",
    "    dist2 = labs[i]**2 + (lord[1] - 1)**2\n",
    "    if dist2 < d2min:\n",
    "      imin = i\n",
    "      d2min = dist2\n",
    "  return imin\n",
    "\n",
    "# get indices of empirical xi\n",
    "iminority = find_xi(store_error[ : , 0], store_error[ : , 1])\n",
    "imajority = find_xi(store_error[ : , 2], store_error[ : , 3])\n",
    "xi_theoric = (cs[2]+cs[3]) - (cs[1] + cs[0]) \n",
    "print(\"xi theorique = \", xi_theoric )\n",
    "print(\"xi minorite = \", xi_loop[iminority])\n",
    "print(\"xi majorite = \", xi_loop[imajority])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#print(store_error)\n",
    "# DISPLAY ROC CURVE \n",
    "#plt.errorbar(xi_loop, store_error[:,0], store_error[:,1])\n",
    "#plt.axvline(cs[1]-cs[0],ls='--',color='m')\n",
    "#plt.xlabel(r'Decision threshold $\\xi$')\n",
    "#plt.ylabel('Misclassification rate')\n",
    "%matplotlib qt\n",
    "plt.plot(store_error[ : , 0 ], store_error[ : , 1 ], 'b', linestyle = '-')\n",
    "plt.plot(store_error[ : , 2], store_error[ : , 3], 'm', linestyle = '-')\n",
    "plt.legend([\"minorit챕\", \"majorit챕\"])\n",
    "plt.xlabel('False Alarm Rate')\n",
    "plt.ylabel('Correct Decision Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNS2PfmcazC4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ROC_curves_fairness_separation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python39564bitbdc9cba0f46f4984b8bada2d6efa70ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
